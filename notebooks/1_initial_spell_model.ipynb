{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class BobRossSegmentedImagesDataset(Dataset):\n",
    "    def __init__(self, dataroot):\n",
    "        super().__init__()\n",
    "        self.dataroot = dataroot\n",
    "        self.imgs = list((self.dataroot / 'train' / 'images').rglob('*.png'))\n",
    "        self.segs = list((self.dataroot / 'train' / 'labels').rglob('*.png'))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)), transforms.ToTensor()\n",
    "        ])\n",
    "        self.color_key = {\n",
    "            3 : 0,\n",
    "            5: 1,\n",
    "            10: 2,\n",
    "            14: 3,\n",
    "            17: 4,\n",
    "            18: 5,\n",
    "            22: 6,\n",
    "            27: 7,\n",
    "            61: 8\n",
    "        }\n",
    "        assert len(self.imgs) == len(self.segs)\n",
    "        # TODO: remean images to N(0, 1)?\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        def translate(x):\n",
    "            return self.color_key[x]\n",
    "        translate = np.vectorize(translate)\n",
    "        \n",
    "        img = Image.open(self.imgs[i])\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        seg = Image.open(self.segs[i])\n",
    "        seg = seg.resize((256, 256))\n",
    "        \n",
    "        # Labels are in the ADE20K ontology and are not consequetive,\n",
    "        # we have to apply a remap operation over the labels in a just-in-time\n",
    "        # manner. This slows things down, but it's fine, this is just a demo\n",
    "        # anyway.\n",
    "        seg = translate(np.array(seg)).astype('int64')\n",
    "        \n",
    "        # One-hot encode the segmentation mask.\n",
    "        # def ohe_mat(segmap):\n",
    "        #     return np.array(\n",
    "        #         list(\n",
    "        #             np.array(segmap) == i for i in range(9)\n",
    "        #         )\n",
    "        #     ).astype(int).reshape(9, 256, 256)\n",
    "        # seg = ohe_mat(seg)\n",
    "        \n",
    "        # Additionally, the original UNet implementation outputs a segmentation map\n",
    "        # for a subset of the overall image, not the image as a whole! With this input\n",
    "        # size the segmentation map targeted is a (164, 164) center crop.\n",
    "        seg = seg[46:210, 46:210]\n",
    "        \n",
    "        return img, seg\n",
    "\n",
    "    \n",
    "from torch import nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_1_1 = nn.Conv2d(3, 64, 3)\n",
    "        self.relu_1_2 = nn.ReLU()\n",
    "        self.conv_1_3 = nn.Conv2d(64, 64, 3)\n",
    "        self.relu_1_4 = nn.ReLU()\n",
    "        self.pool_1_5 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_2_1 = nn.Conv2d(64, 128, 3)\n",
    "        self.relu_2_2 = nn.ReLU()\n",
    "        self.conv_2_3 = nn.Conv2d(128, 128, 3)\n",
    "        self.relu_2_4 = nn.ReLU()        \n",
    "        self.pool_2_5 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_3_1 = nn.Conv2d(128, 256, 3)\n",
    "        self.relu_3_2 = nn.ReLU()\n",
    "        self.conv_3_3 = nn.Conv2d(256, 256, 3)\n",
    "        self.relu_3_4 = nn.ReLU()\n",
    "        self.pool_3_5 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_4_1 = nn.Conv2d(256, 512, 3)\n",
    "        self.relu_4_2 = nn.ReLU()\n",
    "        self.conv_4_3 = nn.Conv2d(512, 512, 3)\n",
    "        self.relu_4_4 = nn.ReLU()\n",
    "        \n",
    "        # deconv is the '2D transposed convolution operator'\n",
    "        self.deconv_5_1 = nn.ConvTranspose2d(512, 256, (2, 2), 2)\n",
    "        # 61x61 -> 48x48 crop\n",
    "        self.c_crop_5_2 = lambda x: x[:, :, 6:54, 6:54]\n",
    "        self.concat_5_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_5_4 = nn.Conv2d(512, 256, 3)\n",
    "        self.relu_5_5 = nn.ReLU()\n",
    "        self.conv_5_6 = nn.Conv2d(256, 256, 3)\n",
    "        self.relu_5_7 = nn.ReLU()\n",
    "        \n",
    "        self.deconv_6_1 = nn.ConvTranspose2d(256, 128, (2, 2), 2)\n",
    "        # 121x121 -> 88x88 crop\n",
    "        self.c_crop_6_2 = lambda x: x[:, :, 17:105, 17:105]\n",
    "        self.concat_6_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_6_4 = nn.Conv2d(256, 128, 3)\n",
    "        self.relu_6_5 = nn.ReLU()\n",
    "        self.conv_6_6 = nn.Conv2d(128, 128, 3)\n",
    "        self.relu_6_7 = nn.ReLU()\n",
    "        \n",
    "        self.deconv_7_1 = nn.ConvTranspose2d(128, 64, (2, 2), 2)\n",
    "        # 252x252 -> 168x168 crop\n",
    "        self.c_crop_7_2 = lambda x: x[:, :, 44:212, 44:212]\n",
    "        self.concat_7_3 = lambda x, y: torch.cat((x, y), dim=1)\n",
    "        self.conv_7_4 = nn.Conv2d(128, 64, 3)\n",
    "        self.relu_7_5 = nn.ReLU()\n",
    "        self.conv_7_6 = nn.Conv2d(64, 64, 3)\n",
    "        self.relu_7_7 = nn.ReLU()\n",
    "        \n",
    "        # 1x1 conv ~= fc; n_classes = 9\n",
    "        self.conv_8_1 = nn.Conv2d(64, 9, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1_1(x)\n",
    "        x = self.relu_1_2(x)\n",
    "        x = self.conv_1_3(x)\n",
    "        x_residual_1 = self.relu_1_4(x)\n",
    "        x = self.pool_1_5(x_residual_1)\n",
    "        \n",
    "        x = self.conv_2_1(x)\n",
    "        x = self.relu_2_2(x)        \n",
    "        x = self.conv_2_3(x)\n",
    "        x_residual_2 = self.relu_2_4(x)        \n",
    "        x = self.pool_2_5(x_residual_2)\n",
    "        \n",
    "        x = self.conv_3_1(x)\n",
    "        x = self.relu_3_2(x)        \n",
    "        x = self.conv_3_3(x)\n",
    "        x_residual_3 = self.relu_3_4(x)\n",
    "        x = self.pool_3_5(x_residual_3)\n",
    "        \n",
    "        x = self.conv_4_1(x)\n",
    "        x = self.relu_4_2(x)\n",
    "        x = self.conv_4_3(x)\n",
    "        x = self.relu_4_4(x)\n",
    "        \n",
    "        x = self.deconv_5_1(x)\n",
    "        x = self.concat_5_3(self.c_crop_5_2(x_residual_3), x)\n",
    "        x = self.conv_5_4(x)\n",
    "        x = self.relu_5_5(x)\n",
    "        x = self.conv_5_6(x)\n",
    "        x = self.relu_5_7(x)\n",
    "        \n",
    "        x = self.deconv_6_1(x)\n",
    "        x = self.concat_6_3(self.c_crop_6_2(x_residual_2), x)\n",
    "        x = self.conv_6_4(x)\n",
    "        x = self.relu_6_5(x)\n",
    "        x = self.conv_6_6(x)\n",
    "        x = self.relu_6_7(x)\n",
    "        \n",
    "        x = self.deconv_7_1(x)\n",
    "        x = self.concat_7_3(self.c_crop_7_2(x_residual_1), x)\n",
    "        x = self.conv_7_4(x)\n",
    "        x = self.relu_7_5(x)\n",
    "        x = self.conv_7_6(x)\n",
    "        x = self.relu_7_7(x)\n",
    "        \n",
    "        x = self.conv_8_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataroot = Path('/spell/bob-ross-kaggle-dataset/')\n",
    "dataset = BobRossSegmentedImagesDataset(dataroot)\n",
    "dataloader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (conv_1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_1_2): ReLU()\n",
       "  (conv_1_3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_1_4): ReLU()\n",
       "  (pool_1_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_2_2): ReLU()\n",
       "  (conv_2_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_2_4): ReLU()\n",
       "  (pool_2_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_3_2): ReLU()\n",
       "  (conv_3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_3_4): ReLU()\n",
       "  (pool_3_5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_4_2): ReLU()\n",
       "  (conv_4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_4_4): ReLU()\n",
       "  (deconv_5_1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv_5_4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_5_5): ReLU()\n",
       "  (conv_5_6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_5_7): ReLU()\n",
       "  (deconv_6_1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv_6_4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_6_5): ReLU()\n",
       "  (conv_6_6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_6_7): ReLU()\n",
       "  (deconv_7_1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv_7_4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_7_5): ReLU()\n",
       "  (conv_7_6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu_7_7): ReLU()\n",
       "  (conv_8_1): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, batch 0. Loss: 2.197.\n",
      "Finished epoch 0, batch 50. Loss: 2.042.\n",
      "Finished epoch 0, batch 100. Loss: 1.882.\n",
      "Finished epoch 0, batch 150. Loss: 3.057.\n",
      "Finished epoch 0, batch 200. Loss: 1.346.\n",
      "Finished epoch 0, batch 250. Loss: 1.727.\n",
      "Finished epoch 0. avg loss: 2.0171173245783347; median loss: 0.8152426481246948\n",
      "Finished epoch 1, batch 0. Loss: 1.833.\n",
      "Finished epoch 1, batch 50. Loss: 1.926.\n",
      "Finished epoch 1, batch 100. Loss: 2.594.\n",
      "Finished epoch 1, batch 150. Loss: 1.427.\n",
      "Finished epoch 1, batch 200. Loss: 1.373.\n",
      "Finished epoch 1, batch 250. Loss: 1.478.\n",
      "Finished epoch 1. avg loss: 1.7515127312139687; median loss: 0.6723160743713379\n",
      "Finished epoch 2, batch 0. Loss: 1.860.\n",
      "Finished epoch 2, batch 50. Loss: 1.713.\n",
      "Finished epoch 2, batch 100. Loss: 1.450.\n",
      "Finished epoch 2, batch 150. Loss: 1.387.\n",
      "Finished epoch 2, batch 200. Loss: 1.533.\n",
      "Finished epoch 2, batch 250. Loss: 2.732.\n",
      "Finished epoch 2. avg loss: 1.7057837000881058; median loss: 0.8094356656074524\n",
      "Finished epoch 3, batch 0. Loss: 1.895.\n",
      "Finished epoch 3, batch 50. Loss: 2.266.\n",
      "Finished epoch 3, batch 100. Loss: 1.113.\n",
      "Finished epoch 3, batch 150. Loss: 1.550.\n",
      "Finished epoch 3, batch 200. Loss: 1.175.\n",
      "Finished epoch 3, batch 250. Loss: 2.048.\n",
      "Finished epoch 3. avg loss: 1.6780154973387242; median loss: 0.6414664387702942\n",
      "Finished epoch 4, batch 0. Loss: 1.524.\n",
      "Finished epoch 4, batch 50. Loss: 2.248.\n",
      "Finished epoch 4, batch 100. Loss: 1.541.\n",
      "Finished epoch 4, batch 150. Loss: 2.461.\n",
      "Finished epoch 4, batch 200. Loss: 1.332.\n",
      "Finished epoch 4, batch 250. Loss: 1.436.\n",
      "Finished epoch 4. avg loss: 1.6842985184069175; median loss: 0.8231624364852905\n",
      "Finished epoch 5, batch 0. Loss: 1.358.\n",
      "Finished epoch 5, batch 50. Loss: 1.060.\n",
      "Finished epoch 5, batch 100. Loss: 1.456.\n",
      "Finished epoch 5, batch 150. Loss: 1.511.\n",
      "Finished epoch 5, batch 200. Loss: 2.500.\n",
      "Finished epoch 5, batch 250. Loss: 1.753.\n",
      "Finished epoch 5. avg loss: 1.6702882977595843; median loss: 0.7676910161972046\n",
      "Finished epoch 6, batch 0. Loss: 1.394.\n",
      "Finished epoch 6, batch 50. Loss: 1.287.\n",
      "Finished epoch 6, batch 100. Loss: 1.586.\n",
      "Finished epoch 6, batch 150. Loss: 1.541.\n",
      "Finished epoch 6, batch 200. Loss: 3.499.\n",
      "Finished epoch 6, batch 250. Loss: 1.543.\n",
      "Finished epoch 6. avg loss: 1.6563205436406383; median loss: 0.8624131679534912\n",
      "Finished epoch 7, batch 0. Loss: 1.440.\n",
      "Finished epoch 7, batch 50. Loss: 1.177.\n",
      "Finished epoch 7, batch 100. Loss: 1.680.\n",
      "Finished epoch 7, batch 150. Loss: 1.922.\n",
      "Finished epoch 7, batch 200. Loss: 1.562.\n",
      "Finished epoch 7, batch 250. Loss: 0.906.\n",
      "Finished epoch 7. avg loss: 1.6499846067086634; median loss: 0.723404586315155\n",
      "Finished epoch 8, batch 0. Loss: 1.489.\n",
      "Finished epoch 8, batch 50. Loss: 2.241.\n",
      "Finished epoch 8, batch 100. Loss: 1.627.\n",
      "Finished epoch 8, batch 150. Loss: 2.347.\n",
      "Finished epoch 8, batch 200. Loss: 2.104.\n",
      "Finished epoch 8, batch 250. Loss: 1.111.\n",
      "Finished epoch 8. avg loss: 1.642648635869957; median loss: 0.5979323983192444\n",
      "Finished epoch 9, batch 0. Loss: 1.657.\n",
      "Finished epoch 9, batch 50. Loss: 1.435.\n",
      "Finished epoch 9, batch 100. Loss: 1.207.\n",
      "Finished epoch 9, batch 150. Loss: 1.359.\n",
      "Finished epoch 9, batch 200. Loss: 1.782.\n",
      "Finished epoch 9, batch 250. Loss: 2.383.\n",
      "Finished epoch 9. avg loss: 1.6186649903833155; median loss: 0.6026478409767151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    \n",
    "    for i, (batch, segmap) in enumerate(dataloader):\n",
    "        \n",
    "        batch = batch.cuda()\n",
    "        segmap = segmap.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, segmap)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        curr_loss = loss.item()\n",
    "        if i % 50 == 0:\n",
    "            print(f'Finished epoch {epoch}, batch {i}. Loss: {curr_loss:.3f}.')\n",
    "        losses.append(curr_loss)\n",
    "    \n",
    "    print(\n",
    "        f'Finished epoch {epoch}. '\n",
    "        f'avg loss: {np.mean(losses)}; median loss: {np.min(losses)}'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
